# service_rag.py
import os
import sqlite3
import logging
from dotenv import load_dotenv
from typing import Optional

# âœ… LangChain + HuggingFace êµ¬ì„±ìš”ì†Œ
from langchain_core.documents import Document
from langchain_text_splitters import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_chroma import Chroma
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.prompts import ChatPromptTemplate
from transformers import pipeline
from openai import OpenAI

# ----------------------------------------------
# ë¡œê¹… ì„¤ì •
# ----------------------------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

# ----------------------------------------------
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env / .env.prod ìë™ ì¸ì‹)
# ----------------------------------------------
env_file = ".env.prod" if os.getenv("ENV") == "production" else ".env"
load_dotenv(dotenv_path=env_file)

CHROMA_PATH = "./chroma_store"
os.makedirs(CHROMA_PATH, exist_ok=True)

# ----------------------------------------------
# API í‚¤ ì„¤ì •
# ----------------------------------------------
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# ----------------------------------------------
# ì„ë² ë”© ëª¨ë¸
# ----------------------------------------------
embedding_model = HuggingFaceEmbeddings(
    model_name=os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
)

# ----------------------------------------------
# SQLite â†’ LangChain ë¬¸ì„œ ë³€í™˜
# ----------------------------------------------
def load_eco_geo_docs():
    docs = []
    conn = sqlite3.connect("TerraMap.db")
    cur = conn.cursor()
    try:
        cur.execute("SELECT region, forest_area, air_quality, biodiversity, eco_score FROM eco_data")
        for row in cur.fetchall():
            docs.append(Document(
                page_content=f"ì§€ì—­ {row[0]}ì˜ ì‚°ë¦¼ë©´ì ì€ {row[1]}, ëŒ€ê¸°ì§ˆì€ {row[2]}, ìƒë¬¼ë‹¤ì–‘ì„±ì€ {row[3]}, ìƒíƒœì ìˆ˜ëŠ” {row[4]}ì…ë‹ˆë‹¤.",
                metadata={"type": "eco"}
            ))

        cur.execute("SELECT city, population_density, traffic_index, green_area, urban_score FROM geo_data")
        for row in cur.fetchall():
            docs.append(Document(
                page_content=f"ë„ì‹œ {row[0]}ì˜ ì¸êµ¬ë°€ë„ëŠ” {row[1]}, êµí†µì§€ìˆ˜ëŠ” {row[2]}, ë…¹ì§€ìœ¨ì€ {row[3]}, ë„ì‹œì„±ì¥ë„ëŠ” {row[4]}ì…ë‹ˆë‹¤.",
                metadata={"type": "geo"}
            ))

        logger.info(f"âœ… {len(docs)}ê°œì˜ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    except sqlite3.Error as e:
        logger.error(f"âš ï¸ SQLite ì˜¤ë¥˜: {e}")
    finally:
        conn.close()
    return docs

# ----------------------------------------------
# ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•
# ----------------------------------------------
def build_vector_store():
    logger.info("ğŸ”§ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
    docs = load_eco_geo_docs()
    if not docs:
        logger.warning("âš ï¸ ë¬¸ì„œ ì—†ìŒ â†’ ì¤‘ë‹¨")
        return None

    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = splitter.split_documents(docs)
    vectordb = Chroma.from_documents(split_docs, embedding_model, persist_directory=CHROMA_PATH)
    vectordb.persist()
    logger.info(f"âœ… {len(split_docs)}ê°œì˜ ë¬¸ì„œê°€ ë²¡í„°í™”ë˜ì–´ ì €ì¥ë¨")
    return vectordb

# ìµœì´ˆ ì‹¤í–‰ ì‹œ ìë™ ìƒì„±
if not os.path.isfile(os.path.join(CHROMA_PATH, "chroma.sqlite3")):
    build_vector_store()

# ----------------------------------------------
# LLM ì´ˆê¸°í™” (CPUìš©)
# ----------------------------------------------
try:
    hf_pipeline = pipeline(
        "text-generation",
        model=os.getenv("GENERATION_MODEL", "mistralai/Mistral-7B-Instruct-v0.2"),
        max_new_tokens=256,
        temperature=0.3,
        device_map=None
    )
    llm = HuggingFacePipeline(pipeline=hf_pipeline)
    logger.info("âœ… Hugging Face LLM ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    logger.error(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    llm = None

# ----------------------------------------------
# RAG ì²´ì¸ êµ¬ì„±
# ----------------------------------------------
try:
    chroma_db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_model)
    retriever = chroma_db.as_retriever()

    prompt = ChatPromptTemplate.from_template("""
    ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤:
    {context}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {input}
    ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ì‹¤ì— ê·¼ê±°í•´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    """)

    doc_chain = create_stuff_documents_chain(llm, prompt)
    qa_chain = create_retrieval_chain(retriever, doc_chain)
    logger.info("âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ")
except Exception as e:
    logger.error(f"âŒ RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    qa_chain = None

# ----------------------------------------------
# OpenAI ë¬¸ì²´ ë³´ì •
# ----------------------------------------------
def refine_with_openai(text: str) -> str:
    if not openai_client:
        return text
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ìì—°ìŠ¤ëŸ½ê³  ë§¤ë„ëŸ½ê²Œ ë¬¸ì¥ì„ ë‹¤ë“¬ëŠ” í¸ì§‘ìì•¼."},
                {"role": "user", "content": f"ë‹¤ìŒ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ì¤˜:\n\n{text}"}
            ],
            temperature=0.5,
            max_tokens=512
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"âš ï¸ OpenAI ë³´ì • ì˜¤ë¥˜: {e}")
        return text

# ----------------------------------------------
# ìµœì¢… ì§ˆì˜ í•¨ìˆ˜
# ----------------------------------------------
def ask_with_rag(query: str) -> str:
    logger.info(f"ğŸ” [RAG] ì§ˆì˜ ìˆ˜ì‹ : {query}")
    if qa_chain is None:
        return "âš ï¸ RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    try:
        result = qa_chain.invoke({"input": query})
        answer = result.get("answer") or result.get("output") or str(result)
        refined = refine_with_openai(answer)
        logger.info("âœ… ì‘ë‹µ ë° ë¬¸ì²´ ë³´ì • ì™„ë£Œ")
        return refined
    except Exception as e:
        logger.error(f"âš ï¸ ì§ˆì˜ ì˜¤ë¥˜: {e}")
        return "âš ï¸ ì§ˆì˜ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
